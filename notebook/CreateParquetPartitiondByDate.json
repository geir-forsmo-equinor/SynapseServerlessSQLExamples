{
	"name": "CreateParquetPartitiondByDate",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "MySpark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7f5e2d4d-ad44-4edf-b644-c3dd1cd4e22a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/687f03ea-b265-4c5d-a6c6-926e951ee79a/resourceGroups/geir-forsmo-sandbox/providers/Microsoft.Synapse/workspaces/tdisynapseworkspace/bigDataPools/MySpark",
				"name": "MySpark",
				"type": "Spark",
				"endpoint": "https://tdisynapseworkspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/MySpark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Make sure you change the start date to today so we can ensure there is data for today."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import NYC yellow cab data from Azure Open Datasets\r\n",
					"from azureml.opendatasets import NycTlcYellow\r\n",
					"\r\n",
					"from datetime import datetime\r\n",
					"from dateutil import parser\r\n",
					"\r\n",
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"# Get the current month and year so we can build out a dataset for partitioning through today\r\n",
					"sqldf = spark.sql(\"select month(current_date()) as month, dayofmonth(current_date()) as day\")\r\n",
					"\r\n",
					"for datecol in sqldf.collect():\r\n",
					"    currentmonth = datecol[\"month\"]\r\n",
					"    currentday = datecol[\"day\"]\r\n",
					"\r\n",
					"#2018 is the last year in the yellowtaxi dataset\r\n",
					"end_date_val = \"2018-\" + str(currentmonth) + \"-\" + str(currentday) + \" 23:59:59\"\r\n",
					"end_date = parser.parse(end_date_val)\r\n",
					"start_date = parser.parse('2018-01-01 00:00:00')\r\n",
					"\r\n",
					"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
					"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"We're using the NYC Yellow Taxi dataset for our baseline dataset.  This dataset only covers up to 2018 in the public samples, so we need to do some manipulations on the file for our purposes:\r\n",
					"1. We add 4 years to the start and end date to make the data appear current.\r\n",
					"2. We add 4 years to the pyYear column to correspond to the manipulated dates.\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"##  Shift the dates from 2018 -> 2022, and create a field with pickup date\r\n",
					"##  This assumes you're running the code in 2022, in the future you may need to adjust this logic.\r\n",
					"nyc_tlc_df = nyc_tlc_df.withColumn('tpepPickupDateTime',add_months(col('tpepPickupDateTime'),48))\r\n",
					"nyc_tlc_df = nyc_tlc_df.withColumn('tpepDropoffDateTime',add_months(col('tpepDropoffDateTime'),48))\r\n",
					"nyc_tlc_df = nyc_tlc_df.withColumn('puYear',col('puYear')+4)\r\n",
					"\r\n",
					"nyc_tlc_df.write.mode(\"overwrite\").partitionBy(\"puYear\",\"puMonth\").parquet(\"/output/nycyellow-ym\")"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"A common approach to partitioning is to partition on year/month/day, where day = day of month.  Here we extract the day from the tpepPickupDateTime to fabricate the day field, and persist the results so we can demonstrate how this can impact query performance."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#This may seem counter intuitive, I'm grabbing the day from the drop off not the pickup.   However, I discovered that the pickup\r\n",
					"#year and month matched the dropoff date not the picup date.\r\n",
					"nyc_df_ymd = nyc_tlc_df.withColumn('puDay', dayofmonth(col('tpepDropoffDateTime')))\r\n",
					"nyc_df_ymd.write.mode(\"overwrite\").partitionBy(\"puYear\",\"puMonth\",\"puDay\").parquet(\"/output/nycyellow-ymd\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"While we do have the attributes for year, month and day in our paritioning structure, to illustrate a secondary concept for partitioning we're going to fabricate a full date field.  This will be used to persist the partitons in year/month/date format where date represents the full date instead of just the calendar day of month."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"nyc_tlc_df = nyc_tlc_df.withColumn(\"puDate\",to_date(col('tpepDropoffDateTime')))\r\n",
					"nyc_tlc_df.write.mode(\"overwrite\").partitionBy(\"puYear\",\"puMonth\",\"puDate\").parquet(\"/output/nycyellow-ymdate\")"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Finally we'll persist the same dataset in delta format instead of parquet to see how this imapcts the Serverless SQL Endpoint."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"nyc_tlc_df.write.mode(\"overwrite\").partitionBy(\"puYear\",\"puMonth\",\"puDate\").format(\"delta\").save(\"/output/nycyellow-ymdate-delta\")"
				],
				"execution_count": 5
			}
		]
	}
}